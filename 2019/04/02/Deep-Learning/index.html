<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="absolutelycold">


    <meta name="subtitle" content="Life, Technology and More">


    <meta name="description" content="Knowledge is Bulletproof.">



<title>Deep-Learning | Feng Talk</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


        
    


</head>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Feng Talk</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Feng Talk</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Deep-Learning</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">absolutelycold</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 2, 2019&nbsp;&nbsp;23:34:34</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <a id="more"></a>
<h3 id="0-Symbol"><a href="#0-Symbol" class="headerlink" title="0. Symbol"></a>0. Symbol</h3><p>$ n $: dimension of x</p>
<p>$ m $: number of samples</p>
<p>$i$: index of samples</p>
<p>$x^{(i)}$: the $x$ of $i^{th}$ sample</p>
<p>$ \alpha $: learning rate</p>
<h3 id="1-Binary-Classification"><a href="#1-Binary-Classification" class="headerlink" title="1. Binary Classification"></a>1. Binary Classification</h3><h4 id="a-Logistic-Regression"><a href="#a-Logistic-Regression" class="headerlink" title="a.Logistic Regression:"></a>a.Logistic Regression:</h4><script type="math/tex; mode=display">
\hat{y} = P(y = 1| x)</script><p>where we want $ 0\leq \hat{y} \leq 1 $.</p>
<p>we can use sigmoid function:</p>
<script type="math/tex; mode=display">
\hat{y} = \sigma(\omega x + b)</script><p>where</p>
<script type="math/tex; mode=display">
\sigma(z) = \frac{1}{1 + e^{-z}}</script><p>To write a common format of activation function $z$, we set $ \Theta $ as follow:</p>
<script type="math/tex; mode=display">
\Theta = 
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
.\\
.\\
.\\
\theta_{n_x}
\end{bmatrix}</script><p>where $ \theta_0 $ is $ b $, </p>
<p>the matrix </p>
<script type="math/tex; mode=display">
\begin{bmatrix}
\theta_1 \\
.\\
.\\
.\\
\theta_{n_x}
\end{bmatrix}</script><p>is $\omega $.</p>
<p>We can write our active function $z$ as $ \Theta^{T}x $</p>
<p>So, the final logistic regression can written as:</p>
<script type="math/tex; mode=display">
\hat{y}^{(i)} = \sigma(\Theta^{T}x^{(i)})</script><h4 id="b-Cost-function"><a href="#b-Cost-function" class="headerlink" title="b. Cost function"></a>b. Cost function</h4><script type="math/tex; mode=display">
L(\hat{y}, y) = -(y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})})</script><p>If $y = 1$, $L(\hat{y}, y) = -\log{\hat{y}}$ and we want lower loss,  $\hat{y}$ will close to 1.</p>
<p>If $y = 0$, $L(\hat{y}, y) = -\log{(1-\hat{y})}$ and we want lower loss, $\hat{y}$ will close to 0.</p>
<p>So, $L(\hat{y}, y)$ can be used as loss function.</p>
<p>Then, we can get the cost function from the cost function:</p>
<script type="math/tex; mode=display">
\begin{aligned}

J(\Theta) 
&= \frac{1}{m}\sum^{m}_{i = 1}L(\hat{y}, y)\\
&= -\frac{1}{m}\sum^{m}_{i = i}[y^{(i)}\log{\hat{y}^{(i)}} + (1 - y^{(i)})\log{(1 - \hat{y}^{(i)}})] 
\end{aligned}</script><h4 id="c-Conclusion"><a href="#c-Conclusion" class="headerlink" title="c. Conclusion"></a>c. Conclusion</h4><p>Logistic Regression:</p>
<script type="math/tex; mode=display">
\hat{y}^{(i)} = \sigma(\Theta^{T}x^{(i)})</script><p>Loss function:</p>
<script type="math/tex; mode=display">
L(\hat{y}, y) = -(y\log{\hat{y}} + (1 - y)\log{(1 - \hat{y})})</script><p>Cost function:</p>
<script type="math/tex; mode=display">
J(\Theta) = -\frac{1}{m}\sum^{m}_{i = i}[y^{(i)}\log{\hat{y}^{(i)}} + (1 - y^{(i)})\log{(1 - \hat{y}^{(i)}})]</script><h3 id="2-Gradient-Descent"><a href="#2-Gradient-Descent" class="headerlink" title="2. Gradient Descent"></a>2. Gradient Descent</h3><p>Repeat {</p>
<p>$\omega = \omega - \alpha\frac{\partial J(\omega, b)}{\partial \omega}$;</p>
<p>$b = b - \alpha\frac{\partial J(\omega, b)}{\partial b}$;</p>
<p>}</p>
<p>当 $\omega$ 和 $b$ 的值几乎不变时，说明到达了最低点。</p>
<p>此时的 $\omega$ 和 $b$ 则是我们需要的最优解。</p>
<h4 id="3-How-to-calculate-the-gradient-descent"><a href="#3-How-to-calculate-the-gradient-descent" class="headerlink" title="3. How to calculate the gradient descent"></a>3. How to calculate the gradient descent</h4><h5 id="Calculate-the-derivation-of-L-hat-y-y"><a href="#Calculate-the-derivation-of-L-hat-y-y" class="headerlink" title="Calculate the derivation of $L(\hat{y}, y)$"></a>Calculate the derivation of $L(\hat{y}, y)$</h5><p>Firstly ,we calculate $ \frac{\partial L(\hat{y}, y)}{\partial \hat{y}} $:</p>
<script type="math/tex; mode=display">
\frac{\partial L(\hat{y}, y)}{\partial \hat{y}} = -\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}</script><p>Then, we calculate $\frac{\partial \hat{y}}{\partial z}$:</p>
<script type="math/tex; mode=display">
\frac{\partial \hat{y}}{\partial z} = \hat{y} (1 - \hat{y})</script><p>Finally, we can get $ \frac{\partial L(\hat{y}, y)}{\partial z }$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial z} 
&= \frac{\partial L(\hat{y}, y)}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \\
&= (-\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}) \cdot (\hat{y} (1 - \hat{y})) \\
&= \hat{y} - y
\end{aligned}</script><p>Then, we can get $\frac{\partial L(\hat{y}, y)}{\partial w_i}$:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L(\hat{y}, y)}{\partial w_i} 
&=  \frac{\partial L(\hat{y}, y)}{\partial z}\cdot \frac{\partial z}{\partial w_i}\\
&= (\hat{y} - y)x_i
\end{aligned}</script><h5 id="Calculate-the-derivation-of-J-w-b"><a href="#Calculate-the-derivation-of-J-w-b" class="headerlink" title="Calculate the derivation of $J(w, b)$"></a>Calculate the derivation of $J(w, b)$</h5><script type="math/tex; mode=display">
\frac{\partial}{\partial w_i} J(w,b) = \frac{1}{m} \sum_{i = 1}^{m} \frac{\partial}{\partial w_i} L(\hat{y}^{(i)},y^{(i)})</script><p>Algorithm to calculate it:</p>
<p>$J=0;\partial w_1 = 0; \partial w_2 = 0; …;\partial w_n = 0 $</p>
<p>$For\ i = 0\ to\ m: $</p>
<script type="math/tex; mode=display">
z^{(i)} = w^T x^{(i)} + b\\
a^{(i)} = \sigma(z^{(i)}) \\
J+= -[y^{(i)} log{a^{(i)}} + (1 - y^{(i)})log{(1 - a^{(i)})}] \\
\partial z^{(i)} = a^{(i)} - y^{(y)} \\
\partial w_1 += x^{(i)}_1 \partial z^{(i)} \\
\partial w_2 += x^{(i)}_2 \partial z^{(i)} \\
\partial w_3 += x^{(i)}_3 \partial z^{(i)} \\
.\\
.\\
.\\
\partial w_n = x^{(i)}_n \partial z^{(i)}\\
\partial b += \partial x^{(i)}</script><p>$J /= m; \partial w_1 /= m; \partial w_1 /= m; \partial w_2 /= m;…;\partial w_n /= m$</p>
<p>After computing the $\frac{\partial}{\partial w_i} J(w,b)$, we can do the gradient descent</p>
<p>Repeat {</p>
<p>​    $\omega = \omega - \alpha\frac{\partial J(\omega, b)}{\partial \omega}$;</p>
<p>​    $b = b - \alpha\frac{\partial J(\omega, b)}{\partial b}$;    </p>
<p>}</p>
<p>in each repeat, we need calculus $\frac{\partial J}{\partial w}$ and $\frac{\partial J}{\partial b}$ </p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>absolutelycold</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://absolutelycold.github.io/2019/04/02/Deep-Learning/">https://absolutelycold.github.io/2019/04/02/Deep-Learning/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Catch You.</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Deep-Learning/"># Deep-Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2019/04/23/JAVA-Learning/">JAVA SE Note</a>
            
            
            <a class="next" rel="next" href="/2019/03/24/Socket-Programming-in-C/">Socket Programming in C</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© absolutelycold | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
